{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "922935e1",
   "metadata": {},
   "source": [
    "# DL Tutorial Day 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d214593",
   "metadata": {},
   "source": [
    "## Implementing a Neural Network from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83625a29",
   "metadata": {},
   "source": [
    "We want to create a class that will mimic the functionality of the keras neural network class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "565234e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "3264d064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>affordibility</th>\n",
       "      <th>bought_insurance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  affordibility  bought_insurance\n",
       "0   22              1                 0\n",
       "1   25              0                 0\n",
       "2   47              1                 1\n",
       "3   52              0                 0\n",
       "4   46              1                 1"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing data\n",
    "df = pd.read_csv('insurance_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "59a8fd74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>affordibility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.55</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  affordibility\n",
       "15  0.55              1\n",
       "12  0.27              0\n",
       "8   0.62              1\n",
       "18  0.19              0\n",
       "5   0.56              1\n",
       "17  0.58              1"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing the data\n",
    "X_train, X_test, y_train, y_test = tts(df[['age','affordibility']], df['bought_insurance'], test_size = 0.2)\n",
    "X_train_scaled = X_train.copy()\n",
    "X_train_scaled['age'] = X_train_scaled['age']/100\n",
    "X_test_scaled = X_test.copy()\n",
    "X_test_scaled['age'] = X_test_scaled['age']/100\n",
    "\n",
    "X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "ac3c6ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myNN:\n",
    "\n",
    "    # constructor\n",
    "    def __init__(self):\n",
    "        self.w1 = 1\n",
    "        self.w2 = 1\n",
    "        self.bias = 0\n",
    "\n",
    "    # loss function\n",
    "    def logloss(self, yt, yp):\n",
    "        epsilon = 1e-15\n",
    "        yp = np.array([max(i, epsilon) for i in yp])\n",
    "        yp = np.array([min(i, 1-epsilon) for i in yp])\n",
    "        loss = -np.mean(yt*np.log(yp) + (1-yt)*np.log(1-yp))\n",
    "        return loss\n",
    "\n",
    "    # activation function\n",
    "    def sigmoid_numpy(self, x):\n",
    "        return np.array(1/(1+np.exp(-x)))\n",
    "    \n",
    "    # defining the fit method\n",
    "    def fit(self, X, y, learning_rate, epochs):\n",
    "        self.w1, self.w2, self.bias = self.gradient_descent(X['age'], X['affordibility'], y, learning_rate, epochs)\n",
    "    \n",
    "    # prediction function\n",
    "    def prediction_function(self, X_test):\n",
    "        weighted_sum = X_test['age']*self.w1 + X_test['affordibility']*self.w2 + self.bias\n",
    "        probability = self.sigmoid_numpy(weighted_sum)\n",
    "        prediction = [1 if i>=0.5 else 0 for i in probability]\n",
    "        return prediction\n",
    "\n",
    "    # gradient descent algorithm\n",
    "    def gradient_descent(self, age, affordability, y_true, learning_rate, epochs, loss_threshold = 0.5):\n",
    "        w1 = w2 = 1\n",
    "        bias = 0\n",
    "        n = len(age)\n",
    "\n",
    "        # implementing the loop until loss falls below the threshold\n",
    "        for i in range(epochs):\n",
    "            weighted_sum = w1*age + w2*affordability + bias\n",
    "            y_predicted = self.sigmoid_numpy(weighted_sum)\n",
    "\n",
    "            loss = self.logloss(y_true, y_predicted)\n",
    "\n",
    "            w1_derivative = (1/n)*np.dot(np.transpose(age), (y_predicted-y_true))\n",
    "            w2_derivative = (1/n)*np.dot(np.transpose(affordability), (y_predicted-y_true))\n",
    "            bias_derivative = np.mean(y_predicted-y_true)\n",
    "\n",
    "            w1 = w1 - learning_rate*w1_derivative\n",
    "            w2 = w2 - learning_rate*w2_derivative\n",
    "            bias = bias - learning_rate*bias_derivative\n",
    "\n",
    "            if i%100 == 0:\n",
    "                print(f\"Epoch: {i+1} W1: {w1} W2: {w2} Bias: {bias} Loss: {loss}\")\n",
    "\n",
    "            if loss < loss_threshold:\n",
    "                print(\"Solution converged!\")\n",
    "                break\n",
    "        return w1, w2, bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "c00a6b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 W1: 0.9999333655766773 W2: 0.9998196654374881 Bias: -0.00027887661205984384 Loss: 0.8029913001994052\n",
      "Epoch: 101 W1: 0.9934141649567464 W2: 0.9820539298006897 Bias: -0.02778245707287551 Loss: 0.7918400092009024\n",
      "Epoch: 201 W1: 0.9871791026858434 W2: 0.9648176709010559 Bias: -0.054529762134414676 Loss: 0.7813207550419676\n",
      "Epoch: 301 W1: 0.9812255855129265 W2: 0.9481090252840213 Bias: -0.08052815248059784 Loss: 0.7714101268666528\n",
      "Epoch: 401 W1: 0.9755505504343607 W2: 0.9319250101255144 Bias: -0.10578621559674432 Loss: 0.7620841103510653\n",
      "Epoch: 501 W1: 0.970150496122459 W2: 0.9162615818368861 Bias: -0.13031367974043012 Loss: 0.7533182768287282\n",
      "Epoch: 601 W1: 0.965021516421812 W2: 0.9011137012454824 Bias: -0.15412132283056237 Loss: 0.7450879636417802\n",
      "Epoch: 701 W1: 0.9601593353229773 W2: 0.8864754039176106 Bias: -0.17722087783198098 Loss: 0.7373684434722896\n",
      "Epoch: 801 W1: 0.9555593428451117 W2: 0.8723398742192794 Bias: -0.19962493614348817 Loss: 0.7301350808884205\n",
      "Epoch: 901 W1: 0.9512166312938005 W2: 0.8586995217739257 Bias: -0.2213468503953574 Loss: 0.7233634748180822\n",
      "Epoch: 1001 W1: 0.947126031405004 W2: 0.8455460590686298 Bias: -0.2424006379355391 Loss: 0.7170295861199015\n",
      "Epoch: 1101 W1: 0.9432821479378424 W2: 0.8328705790739652 Bias: -0.26280088613943997 Loss: 0.7111098498435039\n",
      "Epoch: 1201 W1: 0.939679394335335 W2: 0.8206636318708591 Bias: -0.28256266052339507 Loss: 0.7055812721477438\n",
      "Epoch: 1301 W1: 0.936312026130651 W2: 0.8089152994142286 Bias: -0.3017014164832008 Loss: 0.7004215121700156\n",
      "Epoch: 1401 W1: 0.9331741728348728 W2: 0.7976152677021118 Bias: -0.32023291532188797 Loss: 0.6956089494089492\n",
      "Epoch: 1501 W1: 0.9302598680988899 W2: 0.7867528957557486 Bias: -0.33817314507985585 Loss: 0.6911227373964215\n",
      "Epoch: 1601 W1: 0.9275630779954318 W2: 0.7763172809467713 Bias: -0.3555382465390795 Loss: 0.686942844595105\n",
      "Epoch: 1701 W1: 0.9250777273163909 W2: 0.7662973203293998 Bias: -0.3723444446439488 Loss: 0.6830500835686989\n",
      "Epoch: 1801 W1: 0.9227977238247899 W2: 0.7566817677463208 Bias: -0.3886079854659862 Loss: 0.6794261295387829\n",
      "Epoch: 1901 W1: 0.9207169804395574 W2: 0.7474592865754501 Bias: -0.4043450787391482 Loss: 0.6760535294707962\n",
      "Epoch: 2001 W1: 0.9188294353646391 W2: 0.738618498070518 Bias: -0.4195718459067538 Loss: 0.6729157028282143\n",
      "Epoch: 2101 W1: 0.9171290702019317 W2: 0.7301480253213415 Bias: -0.4343042735499932 Loss: 0.66999693510468\n",
      "Epoch: 2201 W1: 0.9156099261102709 W2: 0.7220365329201274 Bias: -0.4485581720106745 Loss: 0.6672823651944554\n",
      "Epoch: 2301 W1: 0.914266118090698 W2: 0.7142727624689846 Bias: -0.46234913897631896 Loss: 0.664757967597401\n",
      "Epoch: 2401 W1: 0.9130918474917841 W2: 0.7068455641019361 Bias: -0.4756925277626516 Loss: 0.6624105303804237\n",
      "Epoch: 2501 W1: 0.9120814128385016 W2: 0.6997439242231759 Bias: -0.48860342000567897 Loss: 0.660227629736983\n",
      "Epoch: 2601 W1: 0.9112292190944024 W2: 0.6929569896833345 Bias: -0.5010966024614613 Loss: 0.6581976019031351\n",
      "Epoch: 2701 W1: 0.9105297854702319 W2: 0.686474088628166 Bias: -0.5131865476051027 Loss: 0.6563095131054321\n",
      "Epoch: 2801 W1: 0.909977751893071 W2: 0.68028474826058 Bias: -0.5248873977201097 Loss: 0.654553128134861\n",
      "Epoch: 2901 W1: 0.9095678842490117 W2: 0.6743787097582549 Bias: -0.5362129521739152 Loss: 0.6529188780634851\n",
      "Epoch: 3001 W1: 0.9092950785097412 W2: 0.6687459405862461 Bias: -0.5471766575840391 Loss: 0.651397827547642\n",
      "Epoch: 3101 W1: 0.9091543638495033 W2: 0.6633766444378846 Bias: -0.5577916005910306 Loss: 0.6499816420941975\n",
      "Epoch: 3201 W1: 0.9091409048540956 W2: 0.6582612690285742 Bias: -0.5680705029682815 Loss: 0.6486625556048286\n",
      "Epoch: 3301 W1: 0.9092500029180866 W2: 0.6533905119565895 Bias: -0.5780257188142659 Loss: 0.6474333384578043\n",
      "Epoch: 3401 W1: 0.9094770969205319 W2: 0.6487553248331451 Bias: -0.5876692335891823 Loss: 0.6462872663371838\n",
      "Epoch: 3501 W1: 0.9098177632633437 W2: 0.644346915871352 Bias: -0.5970126647748676 Loss: 0.6452180899755627\n",
      "Epoch: 3601 W1: 0.9102677153502209 W2: 0.640156751110622 Bias: -0.6060672639538244 Loss: 0.6442200059382042\n",
      "Epoch: 3701 W1: 0.9108228025778955 W2: 0.6361765544398588 Bias: -0.6148439201199417 Loss: 0.6432876285431977\n",
      "Epoch: 3801 W1: 0.9114790089053829 W2: 0.6323983065697629 Bias: -0.6233531640497703 Loss: 0.642415962983809\n",
      "Epoch: 3901 W1: 0.9122324510610944 W2: 0.6288142430918121 Bias: -0.631605173578842 Loss: 0.6416003796949835\n",
      "Epoch: 4001 W1: 0.9130793764421072 W2: 0.6254168517492738 Bias: -0.6396097796423863 Loss: 0.6408365899856124\n",
      "Epoch: 4101 W1: 0.9140161607545982 W2: 0.6221988690339109 Bias: -0.6473764729537913 Loss: 0.6401206229412667\n",
      "Epoch: 4201 W1: 0.9150393054395176 W2: 0.6191532762110431 Bias: -0.654914411207274 Loss: 0.6394488035882001\n",
      "Epoch: 4301 W1: 0.916145434922944 W2: 0.6162732948653021 Bias: -0.6622324267033666 Loss: 0.638817732298216\n",
      "Epoch: 4401 W1: 0.9173312937263146 W2: 0.6135523820498016 Bias: -0.6693390343071013 Loss: 0.6382242654050443\n",
      "Epoch: 4501 W1: 0.9185937434677476 W2: 0.6109842251125712 Bias: -0.6762424396590725 Loss: 0.6376654969959603\n",
      "Epoch: 4601 W1: 0.9199297597821061 W2: 0.6085627362658893 Bias: -0.6829505475690268 Loss: 0.6371387418371252\n",
      "Epoch: 4701 W1: 0.921336429184115 W2: 0.6062820469566742 Bias: -0.6894709705302098 Loss: 0.6366415193873443\n",
      "Epoch: 4801 W1: 0.9228109458958675 W2: 0.6041365020892343 Bias: -0.6958110373005288 Loss: 0.6361715388523566\n",
      "Epoch: 4901 W1: 0.9243506086573328 W2: 0.6021206541454623 Bias: -0.7019778015036053 Loss: 0.6357266852301972\n",
      "Epoch: 5001 W1: 0.9259528175360308 W2: 0.6002292572419219 Bias: -0.7079780502091645 Loss: 0.635305006297455\n",
      "Epoch: 5101 W1: 0.9276150707498184 W2: 0.5984572611581945 Bias: -0.7138183124578958 Loss: 0.6349047004861847\n",
      "Epoch: 5201 W1: 0.9293349615147769 W2: 0.5967998053662704 Bias: -0.7195048677010034 Loss: 0.6345241056017531\n",
      "Epoch: 5301 W1: 0.9311101749283996 W2: 0.5952522130866713 Bias: -0.7250437541292398 Loss: 0.6341616883328213\n",
      "Epoch: 5401 W1: 0.9329384848967153 W2: 0.5938099853933128 Bias: -0.7304407768702325 Loss: 0.6338160345059679\n",
      "Epoch: 5501 W1: 0.9348177511125721 W2: 0.5924687953858369 Bias: -0.7357015160365068 Loss: 0.6334858400389834\n",
      "Epoch: 5601 W1: 0.9367459160910707 W2: 0.5912244824452413 Bias: -0.7408313346097886 Loss: 0.6331699025486102\n",
      "Epoch: 5701 W1: 0.9387210022670317 W2: 0.5900730465860228 Bias: -0.7458353861499494 Loss: 0.6328671135703647\n",
      "Epoch: 5801 W1: 0.9407411091584136 W2: 0.5890106429157839 Bias: -0.7507186223194268 Loss: 0.6325764513500327\n",
      "Epoch: 5901 W1: 0.942804410598754 W2: 0.5880335762112142 Bias: -0.7554858002160932 Loss: 0.6322969741684316\n",
      "Epoch: 6001 W1: 0.9449091520409506 W2: 0.587138295617589 Bias: -0.7601414895094339 Loss: 0.632027814163044\n",
      "Epoch: 6101 W1: 0.9470536479340544 W2: 0.5863213894773543 Bias: -0.7646900793765288 Loss: 0.6317681716121372\n",
      "Epoch: 6201 W1: 0.9492362791741829 W2: 0.5855795802920093 Bias: -0.7691357852357427 Loss: 0.6315173096489537\n",
      "Epoch: 6301 W1: 0.9514554906301618 W2: 0.5849097198203048 Bias: -0.7734826552772643 Loss: 0.6312745493754908\n",
      "Epoch: 6401 W1: 0.9537097887440928 W2: 0.5843087843147268 Bias: -0.7777345767906739 Loss: 0.6310392653472533\n",
      "Epoch: 6501 W1: 0.9559977392066672 W2: 0.5837738698973474 Bias: -0.7818952822906236 Loss: 0.6308108814021675\n",
      "Epoch: 6601 W1: 0.9583179647067503 W2: 0.5833021880753558 Bias: -0.7859683554424708 Loss: 0.6305888668085747\n",
      "Epoch: 6701 W1: 0.9606691427544852 W2: 0.5828910613958965 Bias: -0.7899572367903637 Loss: 0.6303727327088717\n",
      "Epoch: 6801 W1: 0.9630500035769441 W2: 0.582537919239295 Bias: -0.7938652292907877 Loss: 0.6301620288369332\n",
      "Epoch: 6901 W1: 0.9654593280851856 W2: 0.5822402937492587 Bias: -0.7976955036550603 Loss: 0.6299563404889512\n",
      "Epoch: 7001 W1: 0.9678959459113893 W2: 0.5819958158982368 Bias: -0.8014511035045957 Loss: 0.6297552857287192\n",
      "Epoch: 7101 W1: 0.9703587335146605 W2: 0.5818022116857812 Bias: -0.8051349503430729 Loss: 0.6295585128097289\n",
      "Epoch: 7201 W1: 0.9728466123539489 W2: 0.5816572984674809 Bias: -0.8087498483498623 Loss: 0.629365697797689\n",
      "Epoch: 7301 W1: 0.9753585471264877 W2: 0.5815589814117961 Bias: -0.8122984889992437 Loss: 0.6291765423782519\n",
      "Epoch: 7401 W1: 0.9778935440700697 W2: 0.5815052500819513 Bias: -0.8157834555100788 Loss: 0.628990771835832\n",
      "Epoch: 7501 W1: 0.9804506493274499 W2: 0.5814941751398944 Bias: -0.8192072271306958 Loss: 0.628808133190426\n",
      "Epoch: 7601 W1: 0.9830289473711366 W2: 0.5815239051692225 Bias: -0.8225721832637737 Loss: 0.6286283934803124\n",
      "Epoch: 7701 W1: 0.9856275594867953 W2: 0.5815926636138873 Bias: -0.8258806074360764 Loss: 0.6284513381793942\n",
      "Epoch: 7801 W1: 0.9882456423135139 W2: 0.5816987458294574 Bias: -0.8291346911178444 Loss: 0.628276769738794\n",
      "Epoch: 7901 W1: 0.9908823864391539 W2: 0.5818405162436604 Bias: -0.8323365373966545 Loss: 0.6281045062430822\n",
      "Epoch: 8001 W1: 0.9935370150490321 W2: 0.5820164056229377 Bias: -0.8354881645105008 Loss: 0.6279343801722459\n",
      "Epoch: 8101 W1: 0.996208782626203 W2: 0.5822249084417324 Bias: -0.8385915092448005 Loss: 0.6277662372611768\n",
      "Epoch: 8201 W1: 0.998896973701618 W2: 0.5824645803512606 Bias: -0.8416484301979422 Loss: 0.6275999354490789\n",
      "Epoch: 8301 W1: 1.001600901652472 W2: 0.5827340357445376 Bias: -0.8446607109199314 Loss: 0.6274353439117808\n",
      "Epoch: 8401 W1: 1.0043199075470874 W2: 0.5830319454144832 Bias: -0.847630062928582 Loss: 0.6272723421704683\n",
      "Epoch: 8501 W1: 1.0070533590346937 W2: 0.5833570343019648 Bias: -0.8505581286076045 Loss: 0.6271108192708574\n",
      "Epoch: 8601 W1: 1.009800649278531 W2: 0.5837080793307083 Bias: -0.8534464839908488 Loss: 0.6269506730272769\n",
      "Epoch: 8701 W1: 1.012561195930712 W2: 0.5840839073260451 Bias: -0.8562966414368365 Loss: 0.6267918093265744\n",
      "Epoch: 8801 W1: 1.0153344401473408 W2: 0.5844833930145653 Bias: -0.8591100521976193 Loss: 0.6266341414871343\n",
      "Epoch: 8901 W1: 1.018119845642406 W2: 0.5849054571017784 Bias: -0.8618881088858803 Loss: 0.6264775896686738\n",
      "Epoch: 9001 W1: 1.0209168977790264 W2: 0.5853490644249834 Bias: -0.8646321478440789 Loss: 0.6263220803288192\n",
      "Epoch: 9101 W1: 1.0237251026966576 W2: 0.5858132221786068 Bias: -0.86734345141933 Loss: 0.6261675457227691\n",
      "Epoch: 9201 W1: 1.0265439864729082 W2: 0.5862969782093556 Bias: -0.870023250147591 Loss: 0.6260139234426478\n",
      "Epoch: 9301 W1: 1.02937309431867 W2: 0.5867994193786003 Bias: -0.8726727248506105 Loss: 0.6258611559934132\n",
      "Epoch: 9401 W1: 1.0322119898052822 W2: 0.5873196699894837 Bias: -0.8752930086489918 Loss: 0.6257091904024271\n",
      "Epoch: 9501 W1: 1.035060254122526 W2: 0.5878568902763278 Bias: -0.8778851888945995 Loss: 0.6255579778600315\n",
      "Epoch: 9601 W1: 1.037917485366252 W2: 0.5884102749539842 Bias: -0.8804503090254331 Loss: 0.6254074733886698\n",
      "Epoch: 9701 W1: 1.0407832978545035 W2: 0.5889790518248567 Bias: -0.8829893703459859 Loss: 0.6252576355382987\n",
      "Epoch: 9801 W1: 1.0436573214710405 W2: 0.5895624804413927 Bias: -0.885503333735991 Loss: 0.625108426106003\n",
      "Epoch: 9901 W1: 1.0465392010351844 W2: 0.5901598508219175 Bias: -0.8879931212903721 Loss: 0.6249598098778996\n",
      "Epoch: 10001 W1: 1.049428595696964 W2: 0.5907704822177522 Bias: -0.8904596178930893 Loss: 0.6248117543915561\n",
      "Epoch: 10101 W1: 1.052325178356583 W2: 0.5913937219296393 Bias: -0.8929036727274984 Loss: 0.6246642297173058\n",
      "Epoch: 10201 W1: 1.0552286351072364 W2: 0.5920289441715587 Bias: -0.8953261007257254 Loss: 0.6245172082569508\n",
      "Epoch: 10301 W1: 1.0581386647003723 W2: 0.592675548980087 Bias: -0.8977276839594773 Loss: 0.6243706645584769\n",
      "Epoch: 10401 W1: 1.0610549780324927 W2: 0.5933329611675194 Bias: -0.9001091729746152 Loss: 0.6242245751455094\n",
      "Epoch: 10501 W1: 1.0639772976526756 W2: 0.5940006293170457 Bias: -0.902471288071726 Loss: 0.6240789183603355\n",
      "Epoch: 10601 W1: 1.0669053572899587 W2: 0.5946780248183204 Bias: -0.9048147205348482 Loss: 0.6239336742194174\n",
      "Epoch: 10701 W1: 1.0698389013998215 W2: 0.5953646409418386 Bias: -0.9071401338104257 Loss: 0.6237888242804054\n",
      "Epoch: 10801 W1: 1.0727776847289952 W2: 0.5960599919505895 Bias: -0.9094481646384771 Loss: 0.6236443515197299\n",
      "Epoch: 10901 W1: 1.0757214718978714 W2: 0.5967636122475007 Bias: -0.9117394241379012 Loss: 0.6235002402199393\n",
      "Epoch: 11001 W1: 1.0786700369998077 W2: 0.5974750555572732 Bias: -0.914014498847753 Loss: 0.6233564758660046\n",
      "Epoch: 11101 W1: 1.0816231632166529 W2: 0.5981938941412199 Bias: -0.9162739517262679 Loss: 0.623213045049874\n",
      "Epoch: 11201 W1: 1.0845806424498432 W2: 0.5989197180438102 Bias: -0.9185183231093264 Loss: 0.62306993538263\n",
      "Epoch: 11301 W1: 1.0875422749664436 W2: 0.5996521343696518 Bias: -0.9207481316299974 Loss: 0.6229271354136358\n",
      "Epoch: 11401 W1: 1.0905078690595422 W2: 0.6003907665896928 Bias: -0.9229638751007314 Loss: 0.6227846345561188\n",
      "Epoch: 11501 W1: 1.0934772407224032 W2: 0.6011352538754796 Bias: -0.9251660313597043 Loss: 0.6226424230186772\n",
      "Epoch: 11601 W1: 1.0964502133358447 W2: 0.6018852504603481 Bias: -0.9273550590827674 Loss: 0.6225004917422414\n",
      "Epoch: 11701 W1: 1.0994266173682956 W2: 0.6026404250264651 Bias: -0.9295313985623889 Loss: 0.6223588323420505\n",
      "Epoch: 11801 W1: 1.1024062900880267 W2: 0.603400460116692 Bias: -0.9316954724549232 Loss: 0.6222174370542501\n",
      "Epoch: 11901 W1: 1.1053890752870634 W2: 0.6041650515702635 Bias: -0.9338476864974927 Loss: 0.6220762986867402\n",
      "Epoch: 12001 W1: 1.108374823016306 W2: 0.604933907981332 Bias: -0.9359884301957087 Loss: 0.6219354105739352\n",
      "Epoch: 12101 W1: 1.1113633893314074 W2: 0.6057067501794537 Bias: -0.938118077483415 Loss: 0.621794766535127\n",
      "Epoch: 12201 W1: 1.114354636048973 W2: 0.606483310731141 Bias: -0.9402369873555951 Loss: 0.621654360836159\n",
      "Epoch: 12301 W1: 1.1173484305126715 W2: 0.6072633334616202 Bias: -0.9423455044755146 Loss: 0.6215141881541544\n",
      "Epoch: 12401 W1: 1.1203446453688415 W2: 0.6080465729959924 Bias: -0.9444439597571676 Loss: 0.6213742435450482\n",
      "Epoch: 12501 W1: 1.1233431583512183 W2: 0.6088327943190058 Bias: -0.9465326709240061 Loss: 0.6212345224137059\n",
      "Epoch: 12601 W1: 1.126343852074412 W2: 0.6096217723526938 Bias: -0.9486119430449366 Loss: 0.6210950204864186\n",
      "Epoch: 12701 W1: 1.1293466138357853 W2: 0.6104132915511572 Bias: -0.9506820690484928 Loss: 0.620955733785587\n",
      "Epoch: 12801 W1: 1.1323513354253758 W2: 0.6112071455117951 Bias: -0.9527433302160773 Loss: 0.6208166586064207\n",
      "Epoch: 12901 W1: 1.13535791294356 W2: 0.6120031366023203 Bias: -0.9547959966551219 Loss: 0.6206777914954894\n",
      "Epoch: 13001 W1: 1.138366246626118 W2: 0.6128010756029221 Bias: -0.9568403277529873 Loss: 0.6205391292309828\n",
      "Epoch: 13101 W1: 1.1413762406764223 W2: 0.6136007813629625 Bias: -0.9588765726123775 Loss: 0.6204006688045385\n",
      "Epoch: 13201 W1: 1.144387803104444 W2: 0.614402080471621 Bias: -0.9609049704690266 Loss: 0.6202624074045171\n",
      "Epoch: 13301 W1: 1.1474008455723204 W2: 0.6152048069419173 Bias: -0.9629257510923781 Loss: 0.6201243424006057\n",
      "Epoch: 13401 W1: 1.1504152832461827 W2: 0.6160088019075726 Bias: -0.964939135169951 Loss: 0.6199864713296468\n",
      "Epoch: 13501 W1: 1.153431034654029 W2: 0.6168139133321935 Bias: -0.9669453346760507 Loss: 0.6198487918825938\n",
      "Epoch: 13601 W1: 1.156448021549374 W2: 0.617619995730272 Bias: -0.9689445532254681 Loss: 0.6197113018925021\n",
      "Epoch: 13701 W1: 1.1594661687804408 W2: 0.6184269098995296 Bias: -0.9709369864127759 Loss: 0.6195739993234771\n",
      "Epoch: 13801 W1: 1.162485404164681 W2: 0.6192345226641367 Bias: -0.9729228221378055 Loss: 0.6194368822604976\n",
      "Epoch: 13901 W1: 1.1655056583683945 W2: 0.6200427066283755 Bias: -0.974902240917873 Loss: 0.6192999489000505\n",
      "Epoch: 14001 W1: 1.1685268647912497 W2: 0.6208513399403192 Bias: -0.976875416187293 Loss: 0.6191631975415061\n",
      "Epoch: 14101 W1: 1.1715489594554993 W2: 0.6216603060651151 Bias: -0.9788425145846906 Loss: 0.6190266265791824\n",
      "Epoch: 14201 W1: 1.174571880899709 W2: 0.622469493567496 Bias: -0.9808036962286192 Loss: 0.6188902344950361\n",
      "Epoch: 14301 W1: 1.1775955700768053 W2: 0.6232787959031247 Bias: -0.9827591149819513 Loss: 0.6187540198519347\n",
      "Epoch: 14401 W1: 1.180619970256266 W2: 0.624088111218437 Bias: -0.9847089187055116 Loss: 0.6186179812874634\n",
      "Epoch: 14501 W1: 1.1836450269303038 W2: 0.6248973421586156 Bias: -0.9866532495013708 Loss: 0.6184821175082225\n",
      "Epoch: 14601 W1: 1.1866706877238553 W2: 0.6257063956833814 Bias: -0.9885922439462435 Loss: 0.618346427284577\n",
      "Epoch: 14701 W1: 1.1896969023082387 W2: 0.6265151828902765 Bias: -0.9905260333153817 Loss: 0.618210909445823\n",
      "Epoch: 14801 W1: 1.1927236223183164 W2: 0.6273236188451312 Bias: -0.9924547437973497 Loss: 0.6180755628757363\n",
      "Epoch: 14901 W1: 1.1957508012730367 W2: 0.6281316224194384 Bias: -0.9943784967000575 Loss: 0.6179403865084739\n",
      "Epoch: 15001 W1: 1.1987783944992056 W2: 0.6289391161343341 Bias: -0.9962974086484079 Loss: 0.6178053793248016\n",
      "Epoch: 15101 W1: 1.2018063590583545 W2: 0.6297460260109297 Bias: -0.9982115917738936 Loss: 0.6176705403486173\n",
      "Epoch: 15201 W1: 1.2048346536765873 W2: 0.630552281426736 Bias: -1.0001211538964812 Loss: 0.6175358686437519\n",
      "Epoch: 15301 W1: 1.207863238677281 W2: 0.6313578149779289 Bias: -1.0020261986990868 Loss: 0.6174013633110216\n",
      "Epoch: 15401 W1: 1.2108920759165267 W2: 0.6321625623472149 Bias: -1.003926825894958 Loss: 0.6172670234855142\n",
      "Epoch: 15501 W1: 1.2139211287211862 W2: 0.6329664621770791 Bias: -1.0058231313882333 Loss: 0.6171328483340893\n",
      "Epoch: 15601 W1: 1.2169503618294824 W2: 0.6337694559481893 Bias: -1.0077152074279754 Loss: 0.6169988370530749\n",
      "Epoch: 15701 W1: 1.219979741333995 W2: 0.6345714878627488 Bias: -1.0096031427559344 Loss: 0.6168649888661472\n",
      "Epoch: 15801 W1: 1.2230092346269803 W2: 0.6353725047325957 Bias: -1.011487022748287 Loss: 0.6167313030223772\n",
      "Epoch: 15901 W1: 1.226038810347909 W2: 0.6361724558718592 Bias: -1.013366929551633 Loss: 0.6165977787944303\n",
      "Epoch: 16001 W1: 1.229068438333149 W2: 0.6369712929939826 Bias: -1.0152429422134288 Loss: 0.6164644154769081\n",
      "Epoch: 16101 W1: 1.2320980895676779 W2: 0.6377689701129354 Bias: -1.0171151368071374 Loss: 0.6163312123848207\n",
      "Epoch: 16201 W1: 1.2351277361387738 W2: 0.6385654434484536 Bias: -1.0189835865522723 Loss: 0.6161981688521787\n",
      "Epoch: 16301 W1: 1.238157351191582 W2: 0.6393606713351283 Bias: -1.0208483619295683 Loss: 0.6160652842306962\n",
      "Epoch: 16401 W1: 1.2411869088864922 W2: 0.6401546141352036 Bias: -1.022709530791458 Loss: 0.615932557888594\n",
      "Epoch: 16501 W1: 1.2442163843582348 W2: 0.6409472341549185 Bias: -1.0245671584680653 Loss: 0.6157999892094993\n",
      "Epoch: 16601 W1: 1.2472457536766595 W2: 0.6417384955642574 Bias: -1.0264213078688762 Loss: 0.6156675775914281\n",
      "Epoch: 16701 W1: 1.2502749938091031 W2: 0.6425283643199644 Bias: -1.0282720395802838 Loss: 0.615535322445849\n",
      "Epoch: 16801 W1: 1.2533040825842747 W2: 0.6433168080916969 Bias: -1.0301194119591623 Loss: 0.6154032231968198\n",
      "Epoch: 16901 W1: 1.256332998657622 W2: 0.6441037961911819 Bias: -1.0319634812226406 Loss: 0.6152712792801914\n",
      "Epoch: 17001 W1: 1.2593617214780988 W2: 0.6448892995042583 Bias: -1.0338043015342167 Loss: 0.6151394901428721\n",
      "Epoch: 17101 W1: 1.2623902312562858 W2: 0.6456732904256852 Bias: -1.035641925086383 Loss: 0.615007855242151\n",
      "Epoch: 17201 W1: 1.2654185089338088 W2: 0.6464557427966073 Bias: -1.037476402179884 Loss: 0.6148763740450721\n",
      "Epoch: 17301 W1: 1.2684465361539865 W2: 0.6472366318445627 Bias: -1.0393077812997502 Loss: 0.6147450460278586\n",
      "Epoch: 17401 W1: 1.2714742952336933 W2: 0.6480159341259398 Bias: -1.041136109188258 Loss: 0.6146138706753791\n",
      "Epoch: 17501 W1: 1.274501769136342 W2: 0.6487936274707701 Bias: -1.0429614309149016 Loss: 0.6144828474806565\n",
      "Epoch: 17601 W1: 1.277528941445981 W2: 0.6495696909297757 Bias: -1.044783789943537 Loss: 0.6143519759444147\n",
      "Epoch: 17701 W1: 1.2805557963424332 W2: 0.6503441047235716 Bias: -1.0466032281967919 Loss: 0.6142212555746585\n",
      "Epoch: 17801 W1: 1.2835823185774502 W2: 0.6511168501939346 Bias: -1.048419786117863 Loss: 0.6140906858862871\n",
      "Epoch: 17901 W1: 1.2866084934518327 W2: 0.6518879097570627 Bias: -1.0502335027297989 Loss: 0.6139602664007366\n",
      "Epoch: 18001 W1: 1.2896343067934788 W2: 0.6526572668587322 Bias: -1.0520444156923703 Loss: 0.6138299966456476\n",
      "Epoch: 18101 W1: 1.2926597449363288 W2: 0.6534249059312862 Bias: -1.0538525613566445 Loss: 0.6136998761545606\n",
      "Epoch: 18201 W1: 1.2956847947001537 W2: 0.6541908123523729 Bias: -1.055657974817326 Loss: 0.6135699044666338\n",
      "Epoch: 18301 W1: 1.2987094433711805 W2: 0.6549549724053675 Bias: -1.0574606899629884 Loss: 0.6134400811263802\n",
      "Epoch: 18401 W1: 1.3017336786834866 W2: 0.6557173732414047 Bias: -1.0592607395242508 Loss: 0.6133104056834252\n",
      "Epoch: 18501 W1: 1.3047574888011604 W2: 0.6564780028429607 Bias: -1.061058155120013 Loss: 0.6131808776922831\n",
      "Epoch: 18601 W1: 1.3077808623011848 W2: 0.6572368499889143 Bias: -1.0628529673017997 Loss: 0.6130514967121493\n",
      "Epoch: 18701 W1: 1.310803788157007 W2: 0.6579939042210398 Bias: -1.0646452055963171 Loss: 0.6129222623067071\n",
      "Epoch: 18801 W1: 1.3138262557227878 W2: 0.6587491558118582 Bias: -1.0664348985462773 Loss: 0.6127931740439495\n",
      "Epoch: 18901 W1: 1.3168482547182803 W2: 0.659502595733805 Bias: -1.0682220737495607 Loss: 0.6126642314960135\n",
      "Epoch: 19001 W1: 1.3198697752143214 W2: 0.6602542156296505 Bias: -1.070006757896799 Loss: 0.6125354342390259\n",
      "Epoch: 19101 W1: 1.3228908076189174 W2: 0.6610040077841276 Bias: -1.0717889768074293 Loss: 0.6124067818529612\n",
      "Epoch: 19201 W1: 1.3259113426638907 W2: 0.6617519650967187 Bias: -1.07356875546428 Loss: 0.6122782739215089\n",
      "Epoch: 19301 W1: 1.328931371392068 W2: 0.6624980810555486 Bias: -1.0753461180467643 Loss: 0.6121499100319487\n",
      "Epoch: 19401 W1: 1.331950885144987 W2: 0.6632423497123466 Bias: -1.077121087962722 Loss: 0.6120216897750377\n",
      "Epoch: 19501 W1: 1.334969875551105 W2: 0.6639847656584231 Bias: -1.0788936878789674 Loss: 0.6118936127449013\n",
      "Epoch: 19601 W1: 1.3379883345144787 W2: 0.6647253240016358 Bias: -1.0806639397506055 Loss: 0.6117656785389355\n",
      "Epoch: 19701 W1: 1.3410062542039078 W2: 0.665464020344289 Bias: -1.0824318648491544 Loss: 0.6116378867577122\n",
      "Epoch: 19801 W1: 1.3440236270425179 W2: 0.6662008507619356 Bias: -1.0841974837895316 Loss: 0.6115102370048938\n",
      "Epoch: 19901 W1: 1.3470404456977665 W2: 0.6669358117830453 Bias: -1.0859608165559438 Loss: 0.6113827288871511\n"
     ]
    }
   ],
   "source": [
    "customModel = myNN()\n",
    "customModel.fit(X_train_scaled, y_train, 0.001, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "9b44b5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 0, 1, 1]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customModel.prediction_function(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "2dba16f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15    1\n",
       "12    0\n",
       "8     1\n",
       "18    0\n",
       "5     1\n",
       "17    1\n",
       "Name: bought_insurance, dtype: int64"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
